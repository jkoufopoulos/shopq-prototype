"""
Proper Temporal Evaluation: Using ACTUAL MailQ Digest System

This script runs Dataset 2 through the ACTUAL digest creation pipeline:
1. Loads 70 emails from Dataset 2
2. Converts to email dict format expected by context_digest.py
3. Runs through generate() -> extracts entities -> categorizes -> formats digest
4. Extracts section assignments from the actual DigestCategorizer
5. Compares against T0 ground truth annotations

This tests the REAL system, not a simplified approximation.
"""

import argparse
import email.utils
import sys
from collections import defaultdict
from datetime import UTC, datetime
from pathlib import Path

import pandas as pd

sys.path.insert(0, str(Path(__file__).parent.parent))

from mailq.classification.enrichment import enrich_entities_with_temporal_decay
from mailq.classification.extractor import HybridExtractor
from mailq.digest.categorizer import DigestCategorizer
from mailq.digest.context_digest import ContextDigest


def parse_rfc2822_date(date_str: str) -> datetime | None:
    """Parse RFC 2822 date string to datetime."""
    try:
        date_tuple = email.utils.parsedate_tz(date_str)
        if date_tuple:
            timestamp = email.utils.mktime_tz(date_tuple)
            return datetime.fromtimestamp(timestamp, tz=UTC)
    except Exception as e:
        print(f"âš ï¸  Failed to parse date '{date_str}': {e}")
    return None


def email_to_dict(row: pd.Series) -> dict:
    """Convert Dataset 2 row to email dict format expected by ContextDigest."""
    received_dt = parse_rfc2822_date(row["received_date"])

    return {
        "id": row["email_id"],
        "thread_id": row["email_id"],  # Each email is its own thread for this test
        "subject": row["subject"],
        "snippet": row["snippet"],
        "from": row["from"],
        "to": "user@test.com",  # Placeholder
        "date": received_dt.isoformat() if received_dt else datetime.now(UTC).isoformat(),
        # These will be populated by the pipeline
        "type": None,
        "attention": None,
        "relationship": "unknown",
    }


def get_ground_truth_section(row: pd.Series, timepoint: str) -> str:
    """Extract ground truth section from T0/T1/T2 columns."""
    prefix = f"{timepoint}_"

    # Check which section has an 'X' marker
    if row.get(f"{prefix}critical") == "X":
        return "critical"
    if row.get(f"{prefix}today") == "X":
        return "today"
    if row.get(f"{prefix}coming_up") == "X":
        return "coming_up"
    if row.get(f"{prefix}worth_knowing") == "X":
        return "worth_knowing"
    if row.get(f"{prefix}everything_else") == "X":
        return "everything_else"
    if row.get(f"{prefix}skip") == "X":
        return "skip"
    return "unknown"


def main():
    parser = argparse.ArgumentParser(description="Evaluate ACTUAL digest system on Dataset 2")
    parser.add_argument(
        "--dataset",
        type=str,
        default="reports/dataset2_nov2-9_70_emails.csv",
        help="Path to Dataset 2 CSV",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="reports/actual_digest_eval_dataset2.md",
        help="Output markdown report path",
    )
    parser.add_argument(
        "--digest-time",
        type=str,
        default="2025-11-10T18:20:07",
        help="Digest creation time (ISO format)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging",
    )
    args = parser.parse_args()

    # Load Dataset 2
    dataset_path = Path(__file__).parent.parent / args.dataset
    if not dataset_path.exists():
        print(f"âŒ Dataset not found: {dataset_path}")
        return 1

    df = pd.read_csv(dataset_path)
    print(f"âœ… Loaded {len(df)} emails from Dataset 2")
    print(f"ðŸ“‚ Dataset: {dataset_path}")
    print()

    # Parse digest creation time
    digest_time = datetime.fromisoformat(args.digest_time)
    if digest_time.tzinfo is None:
        digest_time = digest_time.replace(tzinfo=UTC)

    print(f"â° Digest creation time (T0): {digest_time.isoformat()}")
    print()

    # Convert Dataset 2 to email dict format
    print("ðŸ”„ Converting Dataset 2 to email dict format...")
    emails = [email_to_dict(row) for _, row in df.iterrows()]
    print(f"âœ… Converted {len(emails)} emails")
    print()

    # Initialize actual digest system
    print("ðŸ”„ Initializing MailQ Context Digest system...")
    context_digest = ContextDigest(verbose=args.verbose)
    print("âœ… System initialized")
    print()

    # Run through actual digest pipeline
    print("ðŸ”„ Running emails through ACTUAL digest generation pipeline...")
    print(
        "   (This uses: classification â†’ entity extraction â†’ temporal enrichment â†’ categorization)"
    )
    print()

    try:
        result = context_digest.generate(
            emails=emails,
            timezone="America/New_York",
            client_now=digest_time.isoformat(),
        )

        print("âœ… Digest generation complete!")
        print()
        print("   ðŸ“Š Stats:")
        print(f"      - Entities extracted: {result.get('entities_count', 0)}")
        print(f"      - Featured in digest: {result.get('featured_count', 0)}")
        print(f"      - Word count: {result.get('word_count', 0)}")
        print()

    except Exception as e:
        print(f"âŒ Digest generation failed: {e}")
        import traceback

        traceback.print_exc()
        return 1

    # Now we need to extract entity-level section assignments
    # The issue: context_digest.generate() returns HTML, not entity sections
    # We need to access the categorized entities from the pipeline

    print("âš ï¸  LIMITATION IDENTIFIED:")
    print("   The context_digest.generate() method returns HTML/text, not entity sections.")
    print(
        "   We need to extract entities and run categorization separately to get section assignments."
    )
    print()

    # Alternative approach: Run entity extraction + categorization directly
    print("ðŸ”„ Running alternative approach: Direct entity extraction + categorization...")
    print()

    extractor = HybridExtractor()
    categorizer = DigestCategorizer(verbose=args.verbose)

    # Track results
    results = []
    confusion_matrix = defaultdict(lambda: defaultdict(int))
    errors = []

    for idx, row in df.iterrows():
        email_dict = email_to_dict(row)

        # Extract entities (this is what the pipeline does internally)
        try:
            entities = extractor.extract(
                subject=email_dict["subject"],
                snippet=email_dict["snippet"],
                from_email=email_dict["from"],
                thread_id=email_dict["thread_id"],
                email_id=email_dict["id"],
            )

            # Apply temporal enrichment
            enriched_entities = enrich_entities_with_temporal_decay(entities, now=digest_time)

            # Categorize each entity
            predicted_sections = []
            for entity in enriched_entities:
                section = categorizer.categorize(entity)
                predicted_sections.append(section)

            # For evaluation, take the "highest priority" section
            # Priority: critical > today > coming_up > worth_knowing > everything_else > skip
            section_priority = {
                "critical": 0,
                "today": 1,
                "coming_up": 2,
                "worth_knowing": 3,
                "everything_else": 4,
                "skip": 5,
            }

            if predicted_sections:
                predicted_section = min(
                    predicted_sections, key=lambda s: section_priority.get(s, 99)
                )
            else:
                predicted_section = "skip"  # No entities extracted

        except Exception as e:
            print(f"âš ï¸  Failed to process {row['email_id']}: {e}")
            predicted_section = "error"

        # Get ground truth
        ground_truth = get_ground_truth_section(row, "t0")

        # Check correctness
        is_correct = predicted_section == ground_truth

        # Update confusion matrix
        confusion_matrix[ground_truth][predicted_section] += 1

        # Track errors
        if not is_correct:
            errors.append(
                {
                    "email_id": row["email_id"],
                    "subject": row["subject"][:60],
                    "ground_truth": ground_truth,
                    "predicted": predicted_section,
                    "num_entities": len(entities) if entities else 0,
                }
            )

        # Store result
        results.append(
            {
                "email_id": row["email_id"],
                "subject": row["subject"],
                "predicted_section": predicted_section,
                "ground_truth_section": ground_truth,
                "is_correct": is_correct,
                "num_entities": len(entities) if entities else 0,
            }
        )

    # Calculate metrics
    total_emails = len(results)
    correct_predictions = sum(1 for r in results if r["is_correct"])
    accuracy = correct_predictions / total_emails if total_emails > 0 else 0

    print("âœ… Evaluation complete!")
    print()
    print(f"ðŸ“Š Overall Accuracy: {accuracy:.2%} ({correct_predictions}/{total_emails})")
    print(f"âŒ Errors: {len(errors)}")
    print()

    # Per-section metrics
    sections = ["critical", "today", "coming_up", "worth_knowing", "everything_else", "skip"]
    section_metrics = {}

    for section in sections:
        tp = confusion_matrix[section][section]
        fp = sum(confusion_matrix[other][section] for other in sections if other != section)
        fn = sum(confusion_matrix[section][other] for other in sections if other != section)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        section_metrics[section] = {
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "precision": precision,
            "recall": recall,
            "f1": f1,
        }

    # Print summary
    print("ðŸ“Š Per-Section Performance:")
    for section in sections:
        m = section_metrics[section]
        print(f"   {section.upper()}: P={m['precision']:.1%} R={m['recall']:.1%} F1={m['f1']:.1%}")

    # Generate report
    output_path = Path(__file__).parent.parent / args.output

    with open(output_path, "w") as f:
        f.write("# Actual Digest System Evaluation: Dataset 2 (T0)\n\n")
        f.write(f"**Date**: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n")
        f.write(f"**Dataset**: {args.dataset}\n\n")
        f.write(f"**Digest Time**: {digest_time.isoformat()}\n\n")
        f.write(
            "**System**: âœ… ACTUAL MailQ digest pipeline (HybridExtractor + DigestCategorizer)\n\n"
        )
        f.write("---\n\n")

        f.write("## Overall Metrics\n\n")
        f.write(f"- **Accuracy**: {accuracy:.2%} ({correct_predictions}/{total_emails})\n")
        f.write(f"- **Errors**: {len(errors)}\n\n")

        f.write("## Per-Section Metrics\n\n")
        f.write("| Section | Precision | Recall | F1 Score | TP | FP | FN |\n")
        f.write("|---------|-----------|--------|----------|----|----|----|\n")

        for section in sections:
            m = section_metrics[section]
            f.write(
                f"| {section.upper()} | "
                f"{m['precision']:.2%} | "
                f"{m['recall']:.2%} | "
                f"{m['f1']:.2%} | "
                f"{m['tp']} | "
                f"{m['fp']} | "
                f"{m['fn']} |\n"
            )

        f.write("\n")

        f.write("## Confusion Matrix\n\n")
        f.write("| Ground Truth \\ Predicted | " + " | ".join(s.upper() for s in sections) + " |\n")
        f.write("|" + "|".join(["-" * 25] + ["-" * 10 for _ in sections]) + "|\n")

        for gt in sections:
            row = f"| {gt.upper()} | "
            row += " | ".join(str(confusion_matrix[gt][pred]) for pred in sections)
            row += " |\n"
            f.write(row)

        f.write("\n")

        # Error analysis
        if errors:
            f.write("## Error Analysis\n\n")
            f.write(f"Total errors: {len(errors)}\n\n")

            # Group by pattern
            error_patterns = defaultdict(list)
            for err in errors:
                pattern = f"{err['ground_truth']} â†’ {err['predicted']}"
                error_patterns[pattern].append(err)

            for pattern, pattern_errors in sorted(error_patterns.items(), key=lambda x: -len(x[1])):
                f.write(f"### {pattern} ({len(pattern_errors)} errors)\n\n")
                f.write("| Email ID | Subject | Entities |\n")
                f.write("|----------|---------|----------|\n")

                for err in pattern_errors[:10]:
                    f.write(f"| {err['email_id']} | {err['subject']} | {err['num_entities']} |\n")

                if len(pattern_errors) > 10:
                    f.write(f"\n*...and {len(pattern_errors) - 10} more*\n")
                f.write("\n")

    print(f"ðŸ“„ Full report saved to: {output_path}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
