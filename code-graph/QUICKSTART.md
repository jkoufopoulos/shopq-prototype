# MailQ Code-Graph: 3-Lens Quick Reference

**Problem**: A single "everything graph" turns into TV static. You can't answer targeted questions quickly.

**Solution**: Three complementary lenses. Pick the one you need in the moment.

---

## Quick Navigation

**I want to understand...**

- üéØ **"How does X work?"** ‚Üí [Task-Flow Lens](#1-task-flow-lens-sequence-first) (3 scenarios, ‚â§8 steps each)
- üó∫Ô∏è **"Where does Y belong?"** ‚Üí [Layer Map Lens](#2-layer-map-lens-stable-topology) (categories only, no sprawl)
- üî• **"What needs attention now?"** ‚Üí [Evidence Lens](#3-evidence-lens-whats-risky--hot) (top 12 by real signals)

---

## 1) Task-Flow Lens (Sequence-First)

**Purpose**: "What happens when X occurs?"

**Format**: Short sequence diagrams (‚â§8 boxes) showing:
- The contract at each hop (payload name + version)
- No file names unless directly involved
- One happy path + one error branch

**Available Scenarios**:

| Scenario | File | Answers |
|----------|------|---------|
| **Organize Request** | [TASK_FLOW_ORGANIZE.md](visuals/TASK_FLOW_ORGANIZE.md) | "What happens when `/api/organize` is called?" |
| **Digest Generation** | [TASK_FLOW_DIGEST.md](visuals/TASK_FLOW_DIGEST.md) | "How is the daily digest generated?" |
| **Feedback Learning** | [TASK_FLOW_FEEDBACK.md](visuals/TASK_FLOW_FEEDBACK.md) | "How does user feedback become a rule?" |

**Example Output**:
```
Extension ‚Üí API ‚Üí Rules Engine ‚Üí LLM ‚Üí Database
(‚â§8 steps, shows payloads, <30 seconds to understand)
```

**When to Use**:
- ‚úÖ Onboarding new developers
- ‚úÖ Debugging a specific flow
- ‚úÖ Planning a feature change
- ‚ùå Don't use for: "Show me everything" (use System Diagram instead)

---

## 2) Layer Map Lens (Stable Topology)

**Purpose**: "Where does a thing belong?" (org chart for code)

**Format**: One layered map showing:
- Categories only (no leaf files)
- File counts in each box (e.g., "Rules Engine (7 files)")
- Arrows **only between layers**, never within
- Link out to file lists in docs

**Available Views**:

| View | File | Shows |
|------|------|-------|
| **Layer Map** | [LAYER_MAP.md](visuals/LAYER_MAP.md) | Extension ‚Ä¢ Gateway ‚Ä¢ Pipeline ‚Ä¢ Digest ‚Ä¢ Learning layers |

**Example Output**:
```
üì± Extension (21 files)
  ‚öôÔ∏è Core (2)  üìß Gmail (1)  ü§ñ Classifier (3)  üíæ Cache (1)  ‚ú® Features (3)  üîß Utils (7)

‚Üì

üêç Backend (75 files)
  üåê API (9)  ‚ö° Pipeline (4)  üìä Digest (0)  üìö Learning (5)  üîß Utils (3)
```

**When to Use**:
- ‚úÖ "Where should I add this file?"
- ‚úÖ "What layer handles X?"
- ‚úÖ Understanding system boundaries
- ‚ùå Don't use for: How data flows (use Task-Flow instead)

---

## 3) Evidence Lens (What's Risky / Hot)

**Purpose**: "What should I pay attention to?" (directs focus with real signals)

**Format**: Heat-map graph showing:
- Top 12 nodes by composite score
- Node color = churn (git commits last 30 days)
- Border = incidents / TODOs
- Badge = test coverage %

**Available Views**:

| View | File | Shows |
|------|------|-------|
| **Evidence Heat-Map** | [EVIDENCE_HEATMAP.md](visuals/EVIDENCE_HEATMAP.md) | Top 12 components by activity score |

**Signals Used**:
- ‚úÖ **Git churn**: Commits in last 30 days (from `git log`)
- ‚úÖ **TODO count**: Open TODOs/FIXMEs in file
- ‚ö†Ô∏è **Test coverage**: Not yet implemented
- ‚ö†Ô∏è **Production incidents**: Not yet wired

**Example Output**:
```
üî• api.py           - Score: 90 (45 commits, 0 TODOs)
üî• background.js    - Score: 64 (32 commits, 0 TODOs)
‚ö†Ô∏è context_digest.py - Score: 47 (23 commits, 1 TODO)
```

**When to Use**:
- ‚úÖ Planning refactoring priorities
- ‚úÖ Code review focus
- ‚úÖ Roadmap planning
- ‚ùå Don't use for: Static architecture (use Layer Map instead)

---

## Comparison: When to Use Each Lens

| Question | Lens | Example |
|----------|------|---------|
| "How does classification work?" | üéØ Task-Flow | See TASK_FLOW_ORGANIZE.md |
| "Where do I add a new API endpoint?" | üó∫Ô∏è Layer Map | See LAYER_MAP.md ‚Üí API Gateway category |
| "What file should I refactor next?" | üî• Evidence | See EVIDENCE_HEATMAP.md ‚Üí top scoring files |
| "Show me everything" | Use System Diagram | See SYSTEM_DIAGRAM.md (comprehensive view) |

---

## Maintaining This System

### Auto-Generation

All diagrams are **100% auto-generated** from code. No manual updates needed.

**Regenerate everything**:
```bash
./code-graph/scripts/quick_regen.sh
```

**Or step-by-step**:
```bash
# 1. Generate markdown diagrams
python3 code-graph/scripts/generate_diagrams.py

# 2. Generate HTML interactive versions
python3 code-graph/scripts/generate_diagram_html.py

# 3. Open in browser
open code-graph/visuals/html/index.html
```

### Adding New Task-Flow Scenarios

To add a new scenario (e.g., "Auto-Organize Alarm Flow"):

1. Add method to `generate_diagrams.py`:
   ```python
   def generate_task_flow_alarm(self):
       """Task-flow lens: What happens during alarm trigger"""
       # ... sequence diagram code
   ```

2. Add to `generate_all()`:
   ```python
   ("Task-Flow: Alarm Trigger", "TASK_FLOW_ALARM", self.generate_task_flow_alarm()),
   ```

3. Add to HTML generator `diagrams` list:
   ```python
   {
       "file": "task_flow_alarm.html",
       "markdown": "TASK_FLOW_ALARM.md",
       "title": "Task-Flow: Alarm Trigger",
       "icon": "‚è∞",
       "category": "task-flow",
   }
   ```

4. Regenerate: `./code-graph/scripts/quick_regen.sh`

### Updating Evidence Signals

To add test coverage or incident tracking:

1. Update `_compute_evidence_scores()` in `generate_diagrams.py`
2. Add new signal detection (e.g., parse coverage reports)
3. Update scoring formula
4. Regenerate

---

## File Structure

```
code-graph/
‚îú‚îÄ‚îÄ QUICKSTART.md              ‚Üê You are here
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ generate_diagrams.py   ‚Üê Main generator (markdown)
‚îÇ   ‚îú‚îÄ‚îÄ generate_diagram_html.py ‚Üê HTML converter
‚îÇ   ‚îî‚îÄ‚îÄ quick_regen.sh         ‚Üê One-command regenerate
‚îî‚îÄ‚îÄ visuals/
    ‚îú‚îÄ‚îÄ TASK_FLOW_*.md         ‚Üê Task-flow scenarios
    ‚îú‚îÄ‚îÄ LAYER_MAP.md           ‚Üê Layer topology
    ‚îú‚îÄ‚îÄ EVIDENCE_HEATMAP.md    ‚Üê Hot files
    ‚îú‚îÄ‚îÄ SYSTEM_DIAGRAM.md      ‚Üê Comprehensive (all files)
    ‚îú‚îÄ‚îÄ CORE_LAYERS.md         ‚Üê Layered comprehensive
    ‚îú‚îÄ‚îÄ CLASSIFICATION_FLOW.md ‚Üê Detailed flow
    ‚îî‚îÄ‚îÄ html/
        ‚îú‚îÄ‚îÄ index.html         ‚Üê Interactive entry point
        ‚îî‚îÄ‚îÄ *.html             ‚Üê Interactive diagrams
```

---

## FAQ

**Q: When should I use the comprehensive diagrams (SYSTEM_DIAGRAM.md)?**

A: When you need to see **everything at once** or are exploring broadly. But for targeted questions, use the 3 lenses instead.

**Q: Why are some diagrams so small (‚â§8 steps)?**

A: Cognitive load. Research shows humans can hold ~7 items in working memory. Small diagrams answer questions in <30 seconds.

**Q: How often should I regenerate?**

A: After significant architecture changes or weekly (if actively developing). The Evidence lens should be regenerated more frequently (daily) to track churn.

**Q: Can I customize the evidence scoring?**

A: Yes! Edit `_compute_evidence_scores()` in `generate_diagrams.py`. Current formula:
```python
score = (commits * 2) + (todos * 1.5) + (incidents * 5)
```

**Q: What if I want a different task-flow scenario?**

A: Follow the "Adding New Task-Flow Scenarios" guide above. Keep it ‚â§8 steps!

---

## Credits

**Design Philosophy**: Based on cognitive load research and "information scent" principles. A system should answer "what/where/why now" questions in <30 seconds.

**Inspiration**:
- [C4 Model](https://c4model.com/) (layered architecture views)
- [Simon Brown's Software Architecture for Developers](https://softwarearchitecturefordevelopers.com/)
- Code Climate / CodeScene heat-maps

**Generated**: 2025-11-11 (auto-updated on each regeneration)

**Maintainers**: Auto-maintained by codebase scanning. No manual updates required.
